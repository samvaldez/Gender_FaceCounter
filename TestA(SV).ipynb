{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef77548",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test A: Cientifico de Datos - Prueba Reconocimiento de Genero\n",
    "##Por: Samantha Valdez\n",
    "\n",
    "##Construir el contador de reconocimiento facial no es sencillo, por ello me base en un enfoque de Arquitectura modelo CNN y \n",
    "##Cascada en el que las imagenes pasan por un clasificador, luego las partes que nos interesan de la imagen son extraidas \n",
    "##de esta, creando muestras procedemos a modificar el tamano de la imagen para un mejor analisis y esa imagen modificada \n",
    "##la usamos para evaluar un modelo CNN entrenado\n",
    "\n",
    "##A continuacion importo las librerias que necesito (Tensorflow,Keras, h5py, numpy y OpenCV de python) \n",
    "##para crear un modelo usando una Red Neuronal Convolucional\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, Flatten, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95d8cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Para el procesamiento de la data, necesitare cambiar el tamaño de acuerdo a las especificaciones del modelo que diseñare\n",
    "\n",
    "##Defino los parametros iniciales\n",
    "lr = 1e-2 #es un parametro de ajuste que determina el tamaño del paso en cada iteración\n",
    "batch_size = 32 #numero de ejemplos de entrenamiento pasados en una iteración\n",
    "epochs = 100 #el ciclo a traves del conjunto de datos de entrenamiento completo\n",
    "img_dims = (96,96,3) #especifico las dimensiones de la imagen\n",
    "\n",
    "#lists used to store images and its corresponding labels\n",
    "data = [] \n",
    "labels = []\n",
    "\n",
    "#cargo los archivos\n",
    "image_files = [f for f in glob.glob(r'C:\\Users\\Samantha Valdez\\Desktop\\PythonProjects\\genero' + \"/**/*\",\n",
    "recursive=True) if not os.path.isdir(f)] #almaceno todas las imagenes segun la ruta\n",
    "\n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c02365",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conversion de imagenes a un array  \n",
    "##Como en el procesamiento se espera que la data este en un formato numerico, asi podre poner las imagenes en un array usando \n",
    "##la libreria de Keras\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1])) #Cargando cada imagen y cambiando el tamaño de acuerdo con \n",
    "                                                         #las dimensiones que quiero\n",
    "\n",
    "    image = img_to_array(image) #Convierto las imagenes redimensionadas en un array\n",
    "    data.append(image) #agrego las imagenes a una lista de datos\n",
    "\n",
    "    ##Identifico las categorias (Man=0 y Woman=1)\n",
    "    label = img.split(os.path.sep)[-2] \n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c11df24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ahora normalizare los datos y convirtiendo las etiquetas en un array.\n",
    "##Pre-procesado\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "##Separare la data en porcentajes (uno de entrenamiento y otro de validacion)\n",
    "##Divido el conjunto de datos para el entrenamiento y validacion en una proporción de 80 % de datos de entrenamiento y 20 % de datos de prueba\n",
    "x_train,x_test,y_train,y_test = train_test_split(data, labels,\n",
    "test_size=0.2,random_state=42)\n",
    "\n",
    "##Convierto en etiquetas categoricas \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9a241be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aumento el conjunto de datos\n",
    "##este proceso de aumentar la cantidad y diversidad de datos se utiliza para aumentar la cantidad de datos \n",
    "##ayuda a reducir el sobreajuste al entrenar un modelo\n",
    "aug = ImageDataGenerator(rotation_range=25,   #rango de grados para rotaciones aleatorias\n",
    "                         width_shift_range=0.1, #fraccion del ancho total\n",
    "                         height_shift_range=0.1, #fraccion de la altura total\n",
    "                         shear_range=0.2,        #angulo de corte en sentido antihorario en grados\n",
    "                         zoom_range=0.2,         #rango para zoom aleatorio\n",
    "                         horizontal_flip=True,   #voltear aleatoriamente las entradas horizontalmente \n",
    "                         fill_mode=\"nearest\")    #rellenar Los puntos fuera de los límites de la entrada de acuerdo con \n",
    "                                                 #el modo dado en las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16985a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Defino el modelo convolucional\n",
    "#defino la forma de entrada\n",
    "width = img_dims[0]\n",
    "height = img_dims[1]\n",
    "depth = img_dims[2]\n",
    "inputShape = (height, width, depth) #defino la forma de entrada y sus dimensiones\n",
    "dim = -1\n",
    "\n",
    "#creo el modelo\n",
    "model = Sequential() #creo un modelo secuencia con 5 capas ocultas conv2D con la secuencia de 32,64,64,128 y 256 neuronas ocultas \n",
    "                     #(usando relu activation function, batchnormalization,maxpooling2D y Dropout)\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=dim))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=dim))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=dim))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=dim))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=dim))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) #Aplano los datos (los convierto en un array unidimensional para ingresarlos en la siguiente capa)\n",
    "\n",
    "#Capa densa con 1024 neuronas, funcion de activación relu, normalización por lotes y tasa de abandono del 50%\n",
    "model.add(Dense(1024)) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Capa de salida final con 2 neuronas y funcion de activación sigmoid\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ebd0dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samantha Valdez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\Samantha Valdez\\AppData\\Local\\Temp\\ipykernel_20244\\1038399749.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  h = model.fit_generator(aug.flow(x_train, y_train, batch_size=batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 14s 335ms/step - loss: 1.1480 - accuracy: 0.5906 - val_loss: 17.7847 - val_accuracy: 0.6094\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 1.0584 - accuracy: 0.6109 - val_loss: 21.1604 - val_accuracy: 0.5500\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 0.8480 - accuracy: 0.6570 - val_loss: 8.8204 - val_accuracy: 0.4906\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.7525 - accuracy: 0.7102 - val_loss: 3.6250 - val_accuracy: 0.5656\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.6646 - accuracy: 0.7258 - val_loss: 2.9855 - val_accuracy: 0.6438\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.5890 - accuracy: 0.7523 - val_loss: 0.6506 - val_accuracy: 0.7281\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.5766 - accuracy: 0.7820 - val_loss: 0.8600 - val_accuracy: 0.6562\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.6633 - accuracy: 0.7266 - val_loss: 1.1995 - val_accuracy: 0.6938\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.5444 - accuracy: 0.7914 - val_loss: 1.8906 - val_accuracy: 0.6625\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.5856 - accuracy: 0.7797 - val_loss: 0.6385 - val_accuracy: 0.7844\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.5119 - accuracy: 0.7812 - val_loss: 0.4908 - val_accuracy: 0.7844\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.4429 - accuracy: 0.8180 - val_loss: 0.4112 - val_accuracy: 0.8344\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.4387 - accuracy: 0.8219 - val_loss: 0.3008 - val_accuracy: 0.8969\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.3832 - accuracy: 0.8359 - val_loss: 0.2485 - val_accuracy: 0.9031\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.4117 - accuracy: 0.8367 - val_loss: 0.4591 - val_accuracy: 0.8469\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.3621 - accuracy: 0.8625 - val_loss: 0.1815 - val_accuracy: 0.9344\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.3144 - accuracy: 0.8750 - val_loss: 0.3138 - val_accuracy: 0.8844\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.3723 - accuracy: 0.8602 - val_loss: 0.2472 - val_accuracy: 0.9281\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.3646 - accuracy: 0.8562 - val_loss: 0.1826 - val_accuracy: 0.9469\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.2933 - accuracy: 0.8789 - val_loss: 0.2097 - val_accuracy: 0.9187\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 13s 317ms/step - loss: 0.4136 - accuracy: 0.8266 - val_loss: 0.5200 - val_accuracy: 0.8125\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 13s 313ms/step - loss: 0.4041 - accuracy: 0.8328 - val_loss: 0.3534 - val_accuracy: 0.8531\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.3575 - accuracy: 0.8539 - val_loss: 0.3019 - val_accuracy: 0.9031\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 13s 316ms/step - loss: 0.3238 - accuracy: 0.8719 - val_loss: 0.2423 - val_accuracy: 0.9094\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 13s 312ms/step - loss: 0.3247 - accuracy: 0.8727 - val_loss: 0.2381 - val_accuracy: 0.8938\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 13s 314ms/step - loss: 0.3126 - accuracy: 0.8758 - val_loss: 0.1829 - val_accuracy: 0.9250\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.2559 - accuracy: 0.9055 - val_loss: 0.2511 - val_accuracy: 0.9062\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.3123 - accuracy: 0.8773 - val_loss: 0.2456 - val_accuracy: 0.8906\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.2395 - accuracy: 0.9023 - val_loss: 0.2489 - val_accuracy: 0.9031\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.3035 - accuracy: 0.8781 - val_loss: 0.1629 - val_accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 13s 315ms/step - loss: 0.2395 - accuracy: 0.9086 - val_loss: 0.1672 - val_accuracy: 0.9469\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2335 - accuracy: 0.9047 - val_loss: 0.1143 - val_accuracy: 0.9500\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.2121 - accuracy: 0.9133 - val_loss: 0.2428 - val_accuracy: 0.9094\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 13s 317ms/step - loss: 0.2319 - accuracy: 0.9055 - val_loss: 0.1520 - val_accuracy: 0.9438\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.2154 - accuracy: 0.9133 - val_loss: 0.1316 - val_accuracy: 0.9500\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2404 - accuracy: 0.9109 - val_loss: 0.1257 - val_accuracy: 0.9469\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.1973 - accuracy: 0.9328 - val_loss: 0.1716 - val_accuracy: 0.9469\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 13s 316ms/step - loss: 0.1894 - accuracy: 0.9266 - val_loss: 0.2001 - val_accuracy: 0.9094\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.2098 - accuracy: 0.9133 - val_loss: 0.1687 - val_accuracy: 0.9531\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.2439 - accuracy: 0.9047 - val_loss: 0.1881 - val_accuracy: 0.9312\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2559 - accuracy: 0.8977 - val_loss: 0.1296 - val_accuracy: 0.9563\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1414 - accuracy: 0.9438 - val_loss: 0.1296 - val_accuracy: 0.9406\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.1804 - accuracy: 0.9281 - val_loss: 0.2048 - val_accuracy: 0.9125\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.1590 - accuracy: 0.9359 - val_loss: 0.1548 - val_accuracy: 0.9531\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.2011 - accuracy: 0.9234 - val_loss: 0.1242 - val_accuracy: 0.9531\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.1661 - accuracy: 0.9250 - val_loss: 0.1031 - val_accuracy: 0.9594\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1336 - accuracy: 0.9523 - val_loss: 0.1283 - val_accuracy: 0.9531\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1749 - accuracy: 0.9328 - val_loss: 0.1134 - val_accuracy: 0.9719\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.1899 - accuracy: 0.9203 - val_loss: 0.1213 - val_accuracy: 0.9469\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1615 - accuracy: 0.9391 - val_loss: 0.1321 - val_accuracy: 0.9531\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.3887 - accuracy: 0.8359 - val_loss: 1.2840 - val_accuracy: 0.5562\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.4759 - accuracy: 0.7719 - val_loss: 0.4264 - val_accuracy: 0.8125\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.3541 - accuracy: 0.8406 - val_loss: 0.3725 - val_accuracy: 0.8375\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.3004 - accuracy: 0.8641 - val_loss: 0.3103 - val_accuracy: 0.8594\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.2523 - accuracy: 0.8945 - val_loss: 0.2560 - val_accuracy: 0.8656\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2201 - accuracy: 0.9047 - val_loss: 0.2265 - val_accuracy: 0.9281\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 319ms/step - loss: 0.2373 - accuracy: 0.8914 - val_loss: 0.2568 - val_accuracy: 0.8906\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.2424 - accuracy: 0.8961 - val_loss: 0.2387 - val_accuracy: 0.9094\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2228 - accuracy: 0.9055 - val_loss: 0.1428 - val_accuracy: 0.9406\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1811 - accuracy: 0.9273 - val_loss: 0.2382 - val_accuracy: 0.9062\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.2028 - accuracy: 0.9297 - val_loss: 0.1743 - val_accuracy: 0.9219\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1744 - accuracy: 0.9297 - val_loss: 0.1246 - val_accuracy: 0.9438\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.1947 - accuracy: 0.9203 - val_loss: 0.1194 - val_accuracy: 0.9563\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.2303 - accuracy: 0.9109 - val_loss: 0.1385 - val_accuracy: 0.9406\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 13s 317ms/step - loss: 0.1672 - accuracy: 0.9352 - val_loss: 0.2517 - val_accuracy: 0.9062\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.2092 - accuracy: 0.9281 - val_loss: 0.1330 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1836 - accuracy: 0.9203 - val_loss: 0.1384 - val_accuracy: 0.9500\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 0.1918 - accuracy: 0.9281 - val_loss: 0.1584 - val_accuracy: 0.9500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.1598 - accuracy: 0.9383 - val_loss: 0.1827 - val_accuracy: 0.9312\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1741 - accuracy: 0.9273 - val_loss: 0.2216 - val_accuracy: 0.9219\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.1935 - accuracy: 0.9242 - val_loss: 0.1173 - val_accuracy: 0.9563\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.1475 - accuracy: 0.9516 - val_loss: 0.1179 - val_accuracy: 0.9719\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.1461 - accuracy: 0.9477 - val_loss: 0.1079 - val_accuracy: 0.9625\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1692 - accuracy: 0.9359 - val_loss: 0.1023 - val_accuracy: 0.9750\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1491 - accuracy: 0.9453 - val_loss: 0.1171 - val_accuracy: 0.9750\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1365 - accuracy: 0.9438 - val_loss: 0.1180 - val_accuracy: 0.9781\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.1421 - accuracy: 0.9484 - val_loss: 0.1472 - val_accuracy: 0.9750\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.1575 - accuracy: 0.9344 - val_loss: 0.1773 - val_accuracy: 0.9656\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 0.1647 - accuracy: 0.9383 - val_loss: 0.2284 - val_accuracy: 0.9219\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.1645 - accuracy: 0.9375 - val_loss: 0.1284 - val_accuracy: 0.9563\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.1415 - accuracy: 0.9445 - val_loss: 0.1979 - val_accuracy: 0.9219\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.1485 - accuracy: 0.9477 - val_loss: 0.1370 - val_accuracy: 0.9719\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1368 - accuracy: 0.9516 - val_loss: 0.1221 - val_accuracy: 0.9688\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1535 - accuracy: 0.9383 - val_loss: 0.1335 - val_accuracy: 0.9781\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.1326 - accuracy: 0.9555 - val_loss: 0.3120 - val_accuracy: 0.9500\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1538 - accuracy: 0.9430 - val_loss: 0.1640 - val_accuracy: 0.9625\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.1102 - accuracy: 0.9633 - val_loss: 0.1941 - val_accuracy: 0.9688\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.1036 - accuracy: 0.9570 - val_loss: 0.1866 - val_accuracy: 0.9656\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1214 - accuracy: 0.9539 - val_loss: 0.1716 - val_accuracy: 0.9688\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1114 - accuracy: 0.9555 - val_loss: 0.2167 - val_accuracy: 0.9406\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1386 - accuracy: 0.9484 - val_loss: 0.2432 - val_accuracy: 0.9219\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.1518 - accuracy: 0.9383 - val_loss: 0.2881 - val_accuracy: 0.9406\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1079 - accuracy: 0.9555 - val_loss: 0.2227 - val_accuracy: 0.9594\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 13s 323ms/step - loss: 0.1019 - accuracy: 0.9680 - val_loss: 0.2179 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1230 - accuracy: 0.9586 - val_loss: 0.2325 - val_accuracy: 0.9219\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 0.1095 - accuracy: 0.9531 - val_loss: 0.2904 - val_accuracy: 0.9312\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1354 - accuracy: 0.9469 - val_loss: 0.1613 - val_accuracy: 0.9625\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.1266 - accuracy: 0.9516 - val_loss: 0.8326 - val_accuracy: 0.7812\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 0.1449 - accuracy: 0.9461 - val_loss: 0.0805 - val_accuracy: 0.9688\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 13s 316ms/step - loss: 0.1225 - accuracy: 0.9539 - val_loss: 0.1731 - val_accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gender_predictor.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gender_predictor.model\\assets\n"
     ]
    }
   ],
   "source": [
    "##Compilo el modelo\n",
    "#Usando el algoritmo de Adam Optimizer\n",
    "opt = tf.keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "##Ajusto el modelo\n",
    "h = model.fit_generator(aug.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        steps_per_epoch=len(x_train) // batch_size,\n",
    "                        epochs=epochs, verbose=1)\n",
    "\n",
    "##guardo el modelo\n",
    "model.save('gender_predictor.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7129426",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargo el modelo de Prediccion de genero\n",
    "#importo las librerias que necesito \n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#cargo el modelo\n",
    "model = load_model(\"gender_predictor.model\")\n",
    "\n",
    "cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#defino las clases\n",
    "classes = ['men','women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f337b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_facecounter(image, m, f, size=0.5): #tomo la imagen\n",
    "    #convierto la imagen en imagen en escala de grises y la guardo en una variable llamada 'gray_scale'\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = cascade.detectMultiScale(gray_image, 1.1,5)  #utilizo el metodo 'detectMultiScale' para detectar objetos de tamaños diferentes en la imagen\n",
    "                                                         #aqui defino que los objetos detectados sean devueltos como una lista de rectangulos y que se \n",
    "                                                         #guarden en la variable 'faces'\n",
    "    #iterando sobre caras\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(image, (x,y), (x+w,y+h), (0,255,0),3) #dibujo un rectangulo de la imagen usando 'cv2.rectangle'\n",
    "\n",
    "        #recorto la imagen, normalizandola y llevando a un array, luego expandiendo su dimension. La guardo en una variable 'res_face'\n",
    "        \n",
    "        cropped_image = np.copy(image[y:y+h,x:x+w]) \n",
    "\n",
    "        ##preproceso las imagenes segun este modelo\n",
    "        res_face = cv2.resize(cropped_image, (96,96))\n",
    "        ## cv2.imshow(\"cropped image\",res_face)\n",
    "        res_face = res_face.astype(\"float\") / 255.0\n",
    "        res_face = img_to_array(res_face)\n",
    "        res_face = np.expand_dims(res_face, axis=0)\n",
    "\n",
    "        ##prediccion del modelo\n",
    "        #paso el resultado de la variable 'res_face' al modelo entrenado y guardo el resultado en la variable 'result'\n",
    "        \n",
    "        result = model.predict(res_face)[0]\n",
    "\n",
    "        ##obtengo la etiqueta con la máxima precisión\n",
    "        idx = np.argmax(result)  #uso 'argmax' para encontrar la clase con la mayor probabilidad predicha por el resultado\n",
    "        label = classes[idx]\n",
    "\n",
    "        ##calculando el conteo \n",
    "        #si el 'label' es igual a 'woman', incremento la variable 'f' sino la variable 'm'\n",
    "        \n",
    "        if label == \"women\":\n",
    "            f = f+1\n",
    "        else:\n",
    "            m = m+1\n",
    "\n",
    "    cv2.rectangle(image,(0,0),(300,30),(255,255,255),-1)\n",
    "    cv2.putText(image, \" females = {},males = {} \".format(f,m),(0,15),\n",
    "    cv2.FONT_HERSHEY_TRIPLEX,0.6,(255, 101, 125),1)\n",
    "    cv2.putText(image, \" faces detected = \" + str(len(faces)),(10,30), #uso este metodo para dibujar un string de la imagen\n",
    "    cv2.FONT_HERSHEY_TRIPLEX,0.5,(0,0,0),1)\n",
    "\n",
    "    return image #muestro los resultados de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "889a0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    }
   ],
   "source": [
    "##Pruebo con una imagen\n",
    "image = cv2.imread(\"C:\\\\Users\\\\Samantha Valdez\\\\Desktop\\\\PythonProjects\\\\genero\\\\woman\\\\face_1028.jpg\") #leo la imagen especificando el nombre en la ruta\n",
    "\n",
    "##mantenimiento de contadores separados para los dos generos\n",
    "males = 0\n",
    "females = 0\n",
    "\n",
    "##ver resultado en ventana emergente\n",
    "cv2.imshow(\"Gender FaceCounter\", gender_facecounter(image,males,females)) #paso la imagen de entrada junto con las variables del contador\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() #finalmente cierro todas las ventanas o pestanas activas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
